agent_type: corridor
# Training parameters
max_episodes: 400
save_interval: 99
num_threads: 4
num_rollouts: 1
save_dir: models
device: auto  # Auto-select: use GPU if CUDA is available; otherwise use CPU
profile_timing: true

# Checkpoint parameters
checkpoint_interval: 99  # Save a checkpoint every 100 episodes (set to 0 to disable)
checkpoint_save_dir: checkpoints
checkpoint_save_models: true
checkpoint_save_training_state: true

# Evaluation parameters
eval_episodes: 5  # Number of episodes to run during evaluation
eval_save_results: true  # Whether to save evaluation results

# Completely disable Corridor Agent (use only the lower-level single-objective agent)
disable_corridor_agent: false  # Set true to disable corridor agent and train single-objective only
# Disable GNN (keep observations consistent with MoLLMLight)
disable_gnn: false  # Set true to disable GNN; this will auto-disable Corridor Agent and use standard observations
max_corridors: 6
max_corridor_length: 5
seed_top_k: 5
corridor_reward_decay: 0.9
corridor_delta_time: 10


# Training monitoring: avg_speed_mps / avg_speed_norm (enable explicitly; may add overhead)
collect_speed_metrics: true

# reward_table logging: average once every 100 steps
reward_log_window: 100
reward_log_flush_partial: true

# GNN
gnn_hidden_dim: 128
gnn_output_dim: 64
gnn_num_layers: 3
gnn_architecture: GAT
gnn_dropout: 0.1
seed_projection_dim: 64

# Grow Module
grow_hidden_dim: 128
grow_max_neighbors: 4

# Intersection Agent
hidden_dim: 128
lambda_friction_coeff: 1.0
lambda_max_initial: 0.5  # Max lambda early in training (0.0-1.0) to avoid overusing the global head; 1.0 = no cap
lambda_mode: advantage_gap  # Paper-aligned: gate uses (standardized) advantage gap Delta
lane_feature_dim: 10
# Use upper-level Grow Q-value as an extra observation for the cooperation-task critic (does not affect the policy)
include_grow_q_in_global_critic: false  # true = append corridor_q_value to value_glob_head global_features (dim +1)
corridor_q_reduce: mean                # max | mean | sum (aggregation when a junction is covered by multiple corridors)

# Cooperative reward shaping (paper-aligned): r^{coop} = r^{loc} + coef * bonus
reward_glob_mode: shaping              # shaping | same_as_loc
corridor_global_reward_coef: 1.0



# PPO/MGDA
ppo_lr: 3e-4
ppo_gamma: 0.99
ppo_clip_ratio: 0.15
ppo_value_coef: 0.5
ppo_entropy_coef: 0.01
ppo_max_grad_norm: 0.5
ppo_epochs: 10
ppo_lam: 0.95
mgda_epsilon: 1e-8

